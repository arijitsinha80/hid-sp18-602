% status: 100
% chapter: Microservices

\title{Kubernetes based Microservices to label Yelp Photos}


\author{Keerthi Naredla}
\affiliation{
  \institution{Indiana University Bloomington}
  \city{Bloomington}
  \state{Indiana}
  \postcode{47404}
  \country{USA}}
\email{knaredla@iu.edu}



% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{Keerthi}



\begin{abstract}
  With the development of advanced technologies such as
  Kubernetes,Docker, Microservices, buildinga large sacale
  application, whcih is considered a tough job, now becomes hand for
  any developer with the basic tools. Using Kubernetes and
  Microservice architecture, developer can focus mainly on the
  application of the product, as portability, deployablity and
  scalability is efficiently taken care by these technologies. This
  project is developed using such advanced concepts of kubernets,
  docker, and microservices to build three-tier architecture of an
  application to annotate photos from Yelp dataset with Cloud Vision
  API, Redis for backend Storage and Google’s Pub/Sub API for
  communication between microservices. The Docker container image
  container technology is used to build microservices pushed to google
  private container registry. In the following section, a brief
  introduction of these concepts and a walthrough of various
  components of this application is discussed.
\end{abstract}

\keywords{hid-sp18-602,docker,vision,kubernetes,yelp,redis,pub/sub}


\maketitle

\section{Introduction}

The applications are growing complex, not just functionality wise but
also the data generated and used are highly increasing. This makes it
absolute necessary to break down such complex application, instead of
a monolithic application so that product deployed is much more
maintainable, scalable, reliable, independent of failure of other
functional components of the application,portable, easy to deploy on
different cloud platforms, and so on. Most of these suit of best
practices called “Twelve-Factor Apps” can be bought into production of
Software applications using technologies such Docker, Kubernetes and
Microservice architecture~\cite{hid-sp18-602-twelve-factor}. In this
project, an application is built using a combination of these
three technologies. In the below subsections, these concepts are
briefly introduced to better understand the working of the
application.


\subsection{Docker}

Modern applications are often built using different technologies with
different versions based on the application requirement. Deploying
many such application on a single Operating System could be a huge
risk at times when there is a requirement of different versions of the
same dependency. For instance, if you consider installing 2 versions
of Nginx on the same OS, there would be a conflict with the namespace,
network port making it logically cumbersome to do so. Although the
concept of containerization is old, the ability to package
applications with their dependencies into the container, making
applications independent and isolated from other applications is huge
progress to deploy. So it becomes very easy to run multiple containers
using Docker. The docker image which is a snapshot of the container is
the basis to build a container. The Docker container image is a
packaging format that contains not only the application but also all
the dependencies and runtime information. These images are hosted on
public or private repository such as docker.hub, google storage.
There are 2 ways to start of with the docker image Dockerfile,
previous docker image. Once the docker image is ready it takes simple
commands listed below to build the container, push the final docker
image to the storage repository, and pull the docker image whenever
required. Thus, it is easy and robust to create, distribute and run
applications using Docker Containers with docker images and docker
command-line tools. There are less or no restrictions for docker
usage, as containers can be built on any machine with Docker installed
it is highly in use by DevOps~\cite{hid-sp18-602-docker}.

\subsection{Kubernetes}

Although Docker makes it easy to deploy and run applications using
container technology when it comes to application configuration,
service discovery, managing updates, secrets management and monitoring
containers on the cluster, a better technology is required to leverage
all of these tasks. Here comes Kubernetes, an open-source platform
that provides a high level of abstraction and orchestration of
containers deployed on one or more clusters, which in turn are treated
as a single logical machine. Usually, a cluster has single Kubernetes
master nodes that keep on running despite explicitly deleting, and
zero or more worker nodes~\cite{hid-sp18-602-kubernetes}. The master
node is responsible for managing the cluster, whereas the worker nodes
work like a VM, it consists of one or more pods, Volume, network ID
and tools to handle container operations. A pod is the smallest unit
of Kubernetes and it consists of one or more containers. All the
containers in the pod have shared the same filesystem and IP address,
this makes the communication between containers in a pod easy. Each of
these pods created based on the scheme which is usually in YAML or
JSON file format. The scheme covers important aspects of spec which
specifies the Pod behavior, container name, container ports. A pod
without Services or Replication Controller cannot be accessed by the
external client, neither scaling and distribution of the application
are possible ~\cite{hid-sp18-602-pods}.

Services provide an external interface for one or more pods. The
Service schema definition has 3 important parameters: kind, metadata,
and spec. The kind is set to Service to indicate a Kubernetes Service,
which is deployment, pod in case of Kubernetes deployment, pod
definition files. The label app and the name constitute the
metadata. The spec mapping includes a ports mapping for port 80 with
name HTTP. The selector is the key mapping in the spec and specifies a
mapping to be used for selecting the Pods to expose via the
Service. Therefore, the service diverts the network traffic to all its
pods with the same label as the label selector specified in the
Service spec, in a round-robin manner. There are 3 different types
of Service: Load Balancer, Internal IP, Node port. If a Service type
is ClusterIP, then the service is accessible only within the cluster
via its internal port. Whereas if the service type is Node port then
the service is accessible from outside the node port, which further
routes the traffic to internal port Cluster IP of the service, that is
automatically created. Similarly Load Balancer service type also
automatically creates Node port and cluster IP. It gives access for
the external user to ping the IP. In addition to this Load Balancer
has the responsibility to balance the load between all the Pods in
Service~\cite{hid-sp18-602-services}. Another important aspect in
scaling applications is Replication Controller, which manages the
replication level of Pods by setting “replicas” in Replication
Controller definition or on the command line with the –replicas
parameter. This ensures that number of Pod replicas are running at any
given time. If a replica fails or is stopped deliberately a new
replica is started automatically. With these 2 crucial features
scaling and replication factor, Kubernetes keep microservices up and
running all the time. Hence, Kubernetes is production-ready, which
provides dynamic container cluster orchestration in real time.

Kubernetes as a cluster manager provides the feasibility for deploying
Microservices by breaking an application into smaller, manageable,
scalable components that could be used by groups with different
requirements; Fault-tolerant cluster in which if a single Pod replica
fails (due to node failure,for example), another is started
automatically; Horizontal scaling in which additional or fewer
replicas of a Pod could be run by just modifying the replicas label in
the Replication Controller or using the replicas parameter in the
kubectl scale command;

\subsection{Google Cloud Platform}

Google Cloud platform gives the flexibility to scale quickly and
handle intense data while having the luxury of not having to maintain
the robust infrastructure, servers, networks etc and create business
solutions. It provides Cloud shell, which comes with a package of a
command-line tool, temporary VM instance of GEC, and access to Google
API with implicit authorization~\cite{hid-sp18-602-cloud-shell}.
Also, it supports language such as Python, Java, Go, PHP,and Ruby.
Moreover, the command-line tool exclusively supports Cloud SDK gcloud
command line tool. The other alternative to Cloud Shell is to download
Google Cloud SDK and enable Authorization through some keys.

\subsection{Google API}

Google API is a set of application programming interface which allows
communication with google services and integration of other
services. It is great tool for developers to perform operations and
use its features easily, google map API, google Visualization API,
good AJAX search are few examples.


\subsubsection{Cloud Pub/Sub API}

Cloud Pub/Sub API is a message passing product that is highly useful
for communication between independent applications hosted on Google
Cloud Platform. The concept of Cloud Pub/Sub has 2 endpoints sender
and receiver and having one instance cloud pub/sub would allow
interaction between many applications. The main advantage of Pub/Sub
compared to other messaging tools like RabbitMQ is it is asynchronous
and decouples publisher from Subscribers, that is any Client who
subscribed as Sender or Publisher can send, Receive messages
irrespective of the client on the other side. In this project psq:
Cloud Pub/Sub, a powerful, scalable and reliable messaging tool,
implemented using Python is used. It has features similar to rq,
simpleq and celery. It forms the basis for communication between
microservices, especially main application and
frontend~\cite{hid-sp18-602-pub/sub}.

\subsubsection{Cloud Vision API}

Cloud Vision API is the most popular API that Google has till date. It
is very easy and efficient to analyze the content of the image, which
has state-of-the art tools for Image detecting features like: face,
text, label and document text,web detection.  It is further made easy
to use, through Cloud AutoML suite. Using Vision AutoML, it is just
one click away to upload images and run pre-determined, custom machine
learning models. It is built based on Google’s powerful technology of
learning-to learn, neural network architecture. In fact, building
custom ML model is just few
steps~\cite{hid-sp18-602-cloud-automl}. First, uploading training
dataset with images labeled into google bucket or human-support to
label images and the ML model is trained according to the provided
dataset. And then test data is passed, and accuracy of prediction,
classification of test data set is determined. However, this feature
of Cloud AutoML is accessible to only limited customers, but the basic
feature of labeling the images such as data in Google is quite
possible through REST API and are available to use in different
programming languages.  Redis is open source in memory database and is
useful as database, cache and message broker. It has different data
structures, remote, persistent and scalable to address wide variety of
problems~\cite{hid-sp18-602-cloud-vision}.

\subsection{Yelp Dataset}

Yelp provides an open-source dataset for the challenge with students
and university grads. It is usually in JSON and CSV format that can be
downloaded from their website. In addition to this Yelp also gives
access to their data through Yelp-Fusion. Yelp Fusion provides REST
API to get access to search, business, metadata. In order to make use
of these REST API Authentication is required, which is recently
modified to the Private Key authentication method, which is a simple 2
step process. Create an account, create manage app, fill in details
and the private key is generated~\cite{hid-sp18-602-yelp}.

\section{Approach}

The main aim of the application is to label photos from Yelp dataset
retrieved on passing location and search term such as food, dinner,
using cloud vision API. The application is divided into 3
microservices frontend, backend, and mainapp. Each of these
functionalities is explained below along with intial setup.

\subsubsection{Initial Setup} 

As mentioned above, 

\TODO{User ref and label from latex, also check grammar in thsi section}

the application requires 2 important API cloud
vision API and Pub/Sub API, which have to enabled for the specific
project id, the application would be started, in google cloud
console. The best part for a software developer to test the working
application is to launch directly using gcloud command-line tool, as
it doesn’t require authentication setup.  For Cloud SDK installed on
the local environment, setting up the authentication is crucial. For
this, it is first required to create a service account and download
service account key which is usually in JSON file format. Then set the
environment variable \verb|GOOGLE_APPLICATION_CREDENTIALS = [PATH]|,
where \verb|PATH| is the file path of the JSON file downloaded from Google
Console Dashboard.

\subsubsection{Frontend Microservices} 

The frontend of the application plays a key role as the load balancer
service for the entire application. It is basically a dynamic
web-page, which allows the user to enter location, for example, San
Francisco, CA and term like food, dinner. Based on these inputs,
photos are fetched. As the frontend service is deployed as a
load-balancer service an external IP is provided which enables user to
access outside of the cluster through a web-browser.

\subsubsection{Backend Microservice} 

The backend of the application is the storage service through Redis
which is important for storage of images and their respective labels
determined using Cloud Vision API~\cite{hid-sp18-602-redis}. The redis
is accessible through redis image specified in backend.yaml file. The
data is stored into redis instance via Mainapp and retrieved in
frontend in order to populate the page with resulting images-label
pair.

\subsubsection{Mainapp Microservice} 

\TODO{Please use verb|code fragments| and not $code fragments$}

The mainapp provides the actual functionality of the application,
starting from scraping the data to generating the desired output. The
major functionalities involved in the service is briefly discussed.
The \verb|yelp_images.py| has 4 functions \verb|query_api()|,
\verb|get_business()|, \verb|search()| and \verb|request()|; As the given location
and term are passed to \verb|query_api|, it sends a GET request and as a
result business id’s of at most 10 businesses are extracted from the
response object. For each business id, a \verb|get_business()| is called
to retrieve business details. From the response object of business
details, photos are extracted. All the photos are returned to the
calling function in main.py.  In vision.py, for image passed, a
post request is sent to Vision API to annotate the image, in
particular, label detection feature is requested. The response object
consists of further details of the image such as score, confidence,
location, and so on but we are mainly interested in the label which is
a description parameter of string type. In storage.py, StrictRedis
class is imported to instantiate redis object. The redis in memory
storage is very useful because of it’s ability to store objects in
key-value pair. Taking advantage of this all labels are stored with
key labels and label-image pair is also stored. This makes very easy
to retrieve data in the frontend service, by just simply looking for
the associated imageurl for the label in the list of
labels~\cite{hid-sp18-602-redis-implementation}.  To summarize,
main.py brings together all the above functionalities, it retrieves
the data from \verb|yelp_images.py|, passes photos to vision.py to label
each one of them and stores using storage.py. As pub/sub enqueues
the whole process in main.py, once the task is done, the frontend
gets triggered.
  
\subsubsection{Pods, Services, Deployments} 

There is one yaml file for each of the microservice, which includes
schema for service as well as deploymenet. The
kubectl create -f <file.yaml> command included in the Makefile
creates the pods,service and deployments. As discussed above the
important parameter in general are type, label selector, replica,
container image and the port where they are exposed.  In this
application the frontend service is created as type load
balancer. This exposes the service outside of the cluster through an
external IP. Once each of the pods are deployed, the application is
production ready. The scalability is maintained with replica factor,
which ensures deletion or failure of one or 2 pods doesn’t stop the
application from running. Also the constant updates are made easy with
rollback.
  
\section{Execution}

The execution of the application is incorporated through
Makefile. Just by running make all inside yelplabel directory, will
spin the docker container images, creation of pods,services and
deployments. By executing the command kubectl get pods list all the
pods created and kubectl get services will show the services up and
running with their given IP address. The external IP of the frontend
service, is the available to curl or run in the browser. This is also
the load-balancer for the application, so on request the traffic is
diverted to other services within the internal port and the entire
application is up and running.
  
\subsubsection{Challenges} 

Selecting the right technologies is adebatable in learning process,
this application can be implemented with Google Bucket Storage instead
of Redis as it also satisfies with key-value kind of storage
system. The debug aspect is the challenge because of the layers of
abstraction over the microservices, the deployments and everything
works fine if there exists an error within one of the microservices,
which gets hard to figure out at times. To an extent we can make use
of kubectl logs and google cloud provide error report.
  
\section{Results}

The results of the project is to display labeled images from Yelp
photos dataset. And this ahiecved by populating the browser with the
label and imageurl pair retrieved from redis. As the cloud-vision api
is a pre-trained model on huge dataset of Google, the label detection
is done, in less amount of time and with high-accuracy rate in terms
of average score is 0.75. For this application, only photos of top 10
business for the search results are choosen and the image.annotate
request is reiterated for 3 times to make sure best label suitable
label is detected for the image, if not in a single request.

\subsection{Benchmark}

Deploying the application is made easy with the use of Docker and
Kubernetes.  The Makefile and Docker file included installs all the
necessary dependecies to creating pods,services and deployments. This
takes around 2-3 minutes to generate the external IP address.The
runtime analysis of the application, depends on the dataset volume for
the given input, as a result it takes few minutes for the
label-detection and showcasing the results on the browser. The
benchmark for this project extensively depends on Cloud Vision API and
Redis Storage. As Redis operates as in-memory database with key-value
storage it servers the applications purpose in retrieving,storing
labels,imageurl in efficient amount of time. Whereas the Cloud vision
API although it has good reputation to show accurate results, the bath
processing for the images is limited. Moroever, the challenge with
yelp photos is to able to distinguish between different varities of in
food catgoery, for instance.

\section{Conclusion}

Thus application to scrape data from yelp-fust thion API and detect label
using Cloud Vision api, which is neatly displayed on a browser with
the support of redis storage technology, follows MVC architecture
workflow which important dier in application deployment. With
Kubernetes not just orchestration of docker components but the
flexibility, scalability for the deployment of microservices is highly
achieved.
  
\section{Future Scope}

I attempted to make use of Cloud AutoML, Vision API to label MNIST
dataset as a part of this project, but unfortunately teh cloud AutoML
gives access to specific user to customize the pre-trained ML model
based on Google Cloud Vision. This is a huge dataset of NIST
authorized handwritten dataset, highly used for testing the accuracy
of ML models for Computer Vision and Image Analysis. Using the same or
more technology stack, I would like to work on MNIST dataset to detect
and label handwritten images as side-project,irrespective of my class
schedule, during upcoming semester with the guidance of the professor
Dr.Gregor von Laszewski.

\begin{acks}

  The authors would like to thank Dr.Gregor von Laszewski for his
  support and suggestions to write this paper.
  
\end{acks}

\bibliographystyle{ACM-Reference-Format}

\bibliography{report}
